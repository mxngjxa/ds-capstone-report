---
title: "Chinese Language Web Classification Model"
subtitle: "Enhanced Content Filtering for Educational Technology"
author: "Mingjia 'Jacky' Guan"
date: "August 22, 2025"
format:
  beamer:
    incremental: true   
  # revealjs:
  #   theme: solarized
  #   slide-number: true
  #   chalkboard: true
  #   preview-links: auto
  #   logo: figures/logo_northern.jpeg
  #   footer: "CWCM Capstone Project | August 2025"
  #   transition: slide
  #   background-transition: fade
  #   highlight-style: github
  #   code-line-numbers: false
bibliography: references.bib            # Path to BibTeX file
execute:
  echo: true
  eval: false
---

# Introduction {background-color="#1e3a8a"}

## The Digital Education Challenge

### Current State
- **>74%** of US schools now meet FCC requirements for connectivity [@connected_nation_2023_school_connectivity]
- **3:2** student-to-computer ratio, up from 7:1 [@oecd2023pisa]
- **85%** of OECD countries promoting digital strategies [@oecd2023pisa]
- **>1 billion** students adopted digital learning (COVID-19) [@unesco2023gem]

---

### The Problem
- **Media Multitasking** in education setting lowers grades [@RAGAN201478]
- **1 in 3** of students distracted by devices in math lessons [@oecd2024screen]
- **80%** of students multitask on electronic devices while studying [@iza2018students]
- Negative impact on academic performance and cognitive development, aka **brain drain** [@Ward2017]

---

![](figures/m1.png)


## Legacy Filtering Limitations

### Traditional Methods
- **URL Filtering**: Block/allow lists
- **DNS Filtering**: Domain-level blocking  
- **Proxy Filtering**: Intermediary servers

---

### Critical Weaknesses
- Limited granularity (domain-level only)
- Easy circumvention (VPNs, proxies)
- Binary approach (all or nothing)
- No contextual understanding

---

### Example Challenge

Same domain, different content:

```yaml
news-website.com:
  - /education/science-breakthrough ‚úÖ
  - /entertainment/inappropriate ‚ùå  
  - /health/wellness-tips ‚úÖ
  - /adult-content ‚ùå
```

| **Traditional filters** | **CWCM** |
|----------------------|--------------------|
| All or nothing | Content-aware decisions |

---

# Background & Related Work {background-color="#059669"}

## Machine Learning Evolution in NLP

![Evolution of Machine Learning over time](figures/m2.png)

## Model Comparison Framework

| **Approach** | **Advantages** | **Limitations** | **Deployment** |
|--------------|----------------|-----------------|----------------|
| **Logistic Regression** | Fast inference, interpretable | Linear boundaries | ‚úÖ Client-side |
| **LinearSVC** | High-dimensional performance | No probabilities | ‚úÖ Client-side |


## Model Comparison Framework

| **Approach** | **Advantages** | **Limitations** | **Deployment** |
|--------------|----------------|-----------------|----------------|
| **XGBoost** | Non-linear patterns | Resource intensive | ‚ùå Server-only |
| **BERT** | Contextual understanding | Computational cost | ‚ùå Server-only |

---

# Problem Statement {background-color="#dc2626"}

## Current CWCM Limitations

### Performance Issues
- Poor detection of subtle harmful content
- Fails on searches like "I want to eat heroin" (ÊàëÊÉ≥ÂêÉÊµ∑Ê¥õÂõ†)
- Limited contextual understanding
- Outdated training data (2022)

---

### Technical Constraints  
- **Maintain**: <200ms inference time
- **Works**: On school Chromebooks
- **Enhances**: Previous model performance on *Drugs, Tobacco, and Weapons*. 

---

### Target Categories
::: {.columns}

:::: {.column .incremental}
**Harmful** üö´

- Drugs
- Tobacco  
- Weapons
- Adult Content
- Violence
- Self-harm
- etc.

::::


:::: {.column .incremental}
**Safe** ‚úÖ

- Education
- News
- Science
- Arts
- etc.

::::

:::

## Project Objectives

::: {.incremental}
1. **Enhance existing CWCM** with expanded dataset
2. **Improve accuracy** for Drugs, Tobacco, Weapons detection  
3. **Maintain deployment constraints** (client-side, real-time)
4. **Prepare foundation** for CWCM-V2 transformer integration
5. **Statistical validation** of improvements
:::

---

# Methodology {background-color="#7c3aed"}

## Data Collection Strategy

### Phase 1: Data Audit

- Relabeled existing URLs (Pages/Hosts)
- Identified 6 categories with insufficient data
- Initial assessment using Gemini 2.5-Flash
- Relabeled by hand to ensure quality

--- 

### Phase 2: Automated Collection  

- Selenium-based Google search crawler
- Keyword permutation strategy
- Random timing to avoid CAPTCHAs
- Domain extraction for potential host identification

---

### Collection Results

![Label Distribution for black category](figures/3_label_distribution_no_white.png)

---

### Collection Results

| Category | New URLs |
|----------|----------|
| Abortion | 14,056 |
| Drugs | 4,600 |
| Gambling | 4,287 |
| Self-harm | 4,859 |
| Tobacco | 2,248 |
| Weapons | 2,482 |

**Final Dataset**: 429,066 datapoints (76.1% harmless)

## Text Processing Pipeline

![](figures/m3.png)

- Text extracted using production server clone
- Custom dictionary constructed for vectorization
- Vectorization strategy: TF-IDF

## Feature Engineering


### Dictionary Construction
- **TextRank** and **TF-IDF**: Top 200 features per category
- **Jaccard Similarity**: ~0.27 between methods
- **White**: 2500 most relevant features
- **Final Dictionary**: 4,783 features
- **Stopwords, Numbers, and Irrelevant Symbols**: Removed

---

### Custom Preprocessing
- Chinese phrase-based segmentation [@jieba]
- Standardization pipeline
- Category-specific weighting

---

## Preprocessed Text Visualized

![Text Distributions](figures/4_text_length_distributions.png)

---

# Model Training {background-color="#ea580c"}

## Comprehensive Algorithm Evaluation

### Initial Screening: 34+ Algorithms


**Linear Methods**

- Logistic Regression
- LinearSVC, PassiveAggressive  
- Perceptron, SGD Classifier

**Tree-based Methods**  

- XGBoost, LightGBM
- Random Forest, Extra Trees

---

### Initial Screening: 34+ Algorithms


**Others**

- Naive Bayes variants
- Neural Networks (MLP)
- SVM with RBF kernel

**Evaluation Protocol**

- 10-fold cross-validation
- Macro-averaged F1-score
- Statistical significance testing
- Deployment feasibility assessment

---

### Results

![Top 10 Agorithms (Default Parameters)](figures/10_simplified_performance_comparison.png)

---

### Results

![Top 10 Agorithms (Default Parameters)](figures/11_accuracy_vs_weighted_f1_zoomed.png)


---

## Top Model Performance

| **Model** | **Accuracy** | **Macro F1** | **Deployment** |
|-----------|--------------|--------------|----------------|
| **Logistic Regression** ‚≠ê | 0.984 | **0.910** | ‚úÖ Client-side |
| PassiveAggressive | **0.986** | 0.907 | ‚úÖ Client-side |
| XGBoost | 0.985 | 0.905 | ‚ùå Resource heavy |
| LinearSVC | 0.983 | 0.879 | ‚úÖ Client-side |

**Ideal Candidate**: Logistic Regression (best macro F1 + explainability)

## Hyperparameter Optimization

### Search Strategy

- **RandomSearchCV/BayesSearchCV**: Initial exploration with reduced dataset
- **GridSearchCV**: Fine-tuning
- **Custom weights**: Class imbalance handling
- **Multiple Machines/Parallelization**: For Training Speedup

---

## Model Comparison

![XGBoost, LinearSVC, and LogisticRegression](figures/28_v4.9.1_combined_comparison_f1-score.png)

---

## Model Comparison

![Logistic Regression Version Comparison](figures/29_v4.9.2_combined_comparison_f1-score.png)


---

### Final Configuration


```python
LogisticRegression(
    C=220.77,           # Regularization
    penalty='l2',       # Ridge regularization  
    multi_class='multinomial',
    solver='saga',      # Scalability
    max_iter=154       # Early convergence
)
```

---

### Performance Evolution

| Version | F1-Macro | Key Changes |
|---------|----------|-------------|
| V0 | 0.806 | Baseline |
| V2/V3 | 0.896 | High recall optimization |
| V4.4 | 0.904 | Data cleaning |  
| **V5** | **0.918** | Balanced optimization |

**V5**: Production model with optimal precision-recall balance


---

# Results {background-color="#0891b2"}

## Final Model Performance by Category

![V5 Model Performance (Macro-Averaged F1, Recall, and Precision)](figures/26_v5_per_class_metrics_train_test.png)


## Statistical Performance Validation


### Logistic Regression vs

::::{.columns}
::: {.column}

### XGBoost (n=130)

- **Mean F1:** 0.9155 vs 0.9183  
- **t-statistic:** -7.60  
- **p-value:** < 0.000001 ‚≠ê  
- **Cohen's d:** -0.67 (medium effect)

:::

::: {.column}

### LinearSVC (n=66)

- **Mean F1:** 0.9059 vs 0.9183  
- **t-statistic:** -9.05  
- **p-value:** < 0.000001 ‚≠ê  
- **Cohen's d:** -1.11 (large effect)

:::
::::

---

## Statistical Significance

![XGBoost](figures/30_4.9.4-xgb.png)



## Statistical Significance

![LinearSVC](figures/30_4.9.4-svm.png)


---


## Dataset Size Impact Analysis


### Correlation Findings
- **F1 vs log(sample size)**: œÅ = 0.687, p = 0.0095 ‚≠ê
- **Recall vs log(sample size)**: œÅ = 0.698, p = 0.008 ‚≠ê  
- **Precision vs log(sample size)**: œÅ = 0.550, p = 0.052

**Key Insight**: Strong evidence that larger datasets improve performance, especially for recall metrics.


## Model Architecture Insights

### Why Logistic Regression Won

::: {.incremental}
1. **Deployment Efficiency**: Client-side JavaScript compatibility
2. **Interpretability**: Clear probability outputs for threshold tuning  
3. **Generalization**: Consistent train-test performance gaps
4. **Statistical Rigor**: Significant outperformance of alternatives
5. **Production Ready**: Proven stability in resource-constrained environments
:::

---

# Discussion {background-color="#be123c"}

## Key Technical Decisions


### Multinomial vs OVR

:::{.incremental}

- **Chosen**: Multinomial approach
- **Trade-off**: Theoretical limitations and practical gains (overfitting)
- **Note**: IIA assumption, One-Vs-Rest testing in progress
- **All Models**: Heavily reliant on Feature Engineering

:::

---

### Linear or No Linear?

:::{.incremental}

- **Chosen**: Linear (Logistic Regression)  
- **Rationale**: Deployment constraints, consistency
- **Evidence**: Non-linear models (XGBoost) showed overfitting
- **Limitations**: LinearSVC difficult to explain, poor performance from other models. 

:::

## Result Tuning with OVR

![Former Configurations for Softmax Output](figures/eda_recall-fp_old_model.png)


## Production Considerations

### Deployment Architecture

![Current and Future Model Deployment Strategy](figures/m4.png)


**Benefits**: No API costs, privacy-first, sub-100ms latency

---

# Ethical Considerations {background-color="#475569"}

## Balancing Safety and Access

### Privacy Protection

- **Client-side processing**: No data leaves device
- **Zero PII collection**: Content analysis only
- **ISO 27001 compliance**: Security standards
- **Transparency**: Clear administrator controls


---

### Bias Mitigation  

- **Cultural sensitivity**: Taiwan-specific content
- **Performance monitoring**: Category-wise analysis
- **Threshold flexibility**: Local customization
- **Continuous evaluation**: Real-world feedback

---

### Content Classification Ethics

**Challenges**

- Subjective nature of "harmful"
- Cultural context dependency
- Over-censorship risks
- Educational access balance

---

**Solutions**  

- Administrator override capabilities
- Category-specific tuning
- Dual-layer architecture (CWCM + V2)
- Regular policy review processes

---

## Public Health Justification

::: {.incremental}
- **WHO warnings**: Teens, screens, and mental health
- **U.S. Surgeon General**: Social media youth advisory  
- **Research evidence**: Violence exposure impacts academic achievement
- **"Brain rot"**: Digital overload cognitive impairment
- **Educational necessity**: Curated digital environments for learning
:::

---

# Future Work {background-color="#166534"}

## CWCM-V2 Development


### Transformer Integration

- **Architecture**: BERT-based secondary layer
- **Purpose**: Contextual understanding enhancement
- **Deployment**: High-precision refinement of CWCM decisions
- **Target**: Complex semantic relationships


---

### Technical Roadmap

1. **Phase 1**: CWCM production deployment  
2. **Phase 2**: Transformer model fine-tuning
3. **Phase 3**: Dual-layer integration
4. **Phase 4**: Performance optimization

---

### Technical Roadmap

![Final Vision](figures/m5.png)

## Research Extensions

### Immediate Improvements
- **Data quality**: Clean non-Chinese entries
- **Dataset expansion**: Systematic collection for underperforming categories
- **Threshold optimization**: Production-based calibration
- **Ensemble methods**: Multiple linear model combinations

---

### Long-term Vision
- **Multilingual support**: Beyond Traditional Chinese
- **Multimodal analysis**: Images and video content
- **Federated learning**: Cross-institutional knowledge sharing
- **Predictive modeling**: Proactive content curation

---

# Conclusion {background-color="#1f2937"}

## Key Achievements

::: {.incremental}
‚úÖ **Enhanced dataset**: 429,066 samples with focused harmful category augmentation

‚úÖ **Statistical validation**: Significant improvements over XGBoost (p<0.000001) and LinearSVC (p<0.000001)

‚úÖ **Production-ready**: Client-side deployment architecture with sub-100ms inference

‚úÖ **Balanced performance**: F1-macro 0.918 across 13 content categories

‚úÖ **Empirical insights**: Strong correlation between dataset size and performance (œÅ=0.687)
:::

## Impact and Contribution


### Educational Technology

- Effective Chinese content filtering for K-12
- Scalable solution for hundreds of thousands of students  
- Privacy-first architecture
- Real-time protection without over-censorship

---

### Contribution

- Systematic evaluation of 34+ ML algorithms for Chinese text classification
- Statistical framework for model selection in educational contexts
- Empirical evidence for data collection prioritization strategies


---

### Technical Innovation

- Custom TF-IDF dictionary construction
- Category-specific threshold calibration
- Client-side transformer preparation
- Dual-layer architecture foundation


---

### Broader Implications  

- Replicable framework for multilingual content filtering
- Balance of accuracy, efficiency, and ethics
- Foundation for next-generation educational AI systems

---

## Thank You {background-color="#064e3b"}

### Questions & Discussion

**Contact Information:**

- Mingjia "Jacky" Guan
- Senior Project - Chinese Web Classification Model
- August 21, 2025

**Key Resources:**

- **GitHub**: Model implementation and documentation
- **Production**: Taiwan K-12 deployment
- **Future**: CWCM-V2 transformer integration

---

### Appendix: Technical Details {.smaller}

#### Model Performance Matrix
```python
# Final Model Configuration
model = LogisticRegression(
    C=220.77,
    penalty='l2', 
    multi_class='multinomial',
    solver='saga',
    class_weight='balanced',
    random_state=42
)

# Performance Metrics
metrics = {
    'f1_macro': 0.9183,
    'precision_macro': 0.9341, 
    'recall_macro': 0.9040,
    'accuracy': 0.9840
}
```

#### Feature Engineering Pipeline
```python
# Custom Dictionary Construction  
def build_feature_dict():
    textrank_features = extract_textrank_top200()
    tfidf_features = extract_tfidf_top200() 
    final_dict = union(textrank_features, tfidf_features)
    return final_dict  # 4,783 total features
```

---